

import numpy as np

def relu(x):
    return np.maximum(0, x)

def sigmoid(x):
    return 1/(1 + np.exp(-x))


class FeedForward2HL:
    def __init__(self):
        # weights only
        self.W1 = np.array([0.5, -0.2, 0.3])
        self.W2 = np.array([0.4, 0.1, -0.5])
        self.Wout = 0.8

    def forward(self, x):
        # Hidden layer 1 → ReLU
        z1 = np.dot(x, self.W1)
        h1 = relu(z1)

        # Hidden layer 2 → Sigmoid
        z2 = np.dot(x, self.W2) * h1
        h2 = sigmoid(z2)

        # Output → Sigmoid
        z3 = h2 * self.Wout
        output = sigmoid(z3)

        return output


# Run example
x = np.array([1, 0, 1])
model = FeedForward2HL()
result = model.forward(x)
print("Final Output:", result)
if result >= 0.5:
    print("Spam")
else:
    print("Not Spam")
     
Final Output: 0.5948395461563475
Spam
